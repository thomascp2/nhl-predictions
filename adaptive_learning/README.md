# Adaptive Learning System

**A self-improving prediction engine that learns from real outcomes**

The adaptive learning system tracks prediction outcomes and automatically discovers probability adjustments that improve accuracy over time. Unlike static models, this system gets smarter every day.

---

## ğŸ¯ What It Does

The adaptive learning system:

1. **Tracks Outcomes** - Records actual results of predictions
2. **Learns Patterns** - Discovers what factors affect accuracy (home ice, player tendencies, etc.)
3. **Applies Adjustments** - Automatically adjusts future predictions based on learnings
4. **Validates Improvements** - Uses statistical significance testing to ensure changes are real

---

## ğŸ“Š What The System Learns

### 1. Home Ice Advantage
- Learns if home players perform better than away players
- Calculates prop-specific boosts (e.g., +3% for home points, +5% for home shots)
- Only applies if statistically significant (p < 0.05)

**Example:**
```
Home points predictions: 72% accuracy
Away points predictions: 65% accuracy
â†’ Learn: +3.5% boost for home games
```

### 2. Player-Specific Calibration
- Identifies players we consistently over/under-predict
- Adjusts probabilities for specific players based on their history
- Min 30 predictions required before calibrating

**Example:**
```
Connor McDavid: Predicted 68% avg, Actual 75% hit rate
â†’ Learn: +3.5% boost for McDavid predictions
```

### 3. Confidence Tier Validation
- Checks if T1-ELITE really hits 75%, T2-STRONG hits 65%, etc.
- Identifies miscalibrated tiers
- Provides feedback for model improvements

### 4. Blowout Game Effects (Coming Soon)
- Learns how large goal margins affect predictions
- Adjusts for garbage time, pulled starters, etc.

### 5. Optimal Ensemble Weights (Coming Soon)
- Learns optimal split between statistical and ML models
- Calculates prop-specific weights (e.g., 65/35 for points, 75/25 for shots)

---

## ğŸš€ Quick Start Guide

### Step 1: Initialize Database

The schema was already applied when you set up the system. To verify:

```bash
sqlite3 database/nhl_predictions.db "SELECT name FROM sqlite_master WHERE type='table' AND name LIKE '%outcome%' OR name LIKE '%learned%';"
```

You should see:
- `prediction_outcomes`
- `learned_adjustments`
- `learning_log`

### Step 2: Grade Yesterday's Predictions

After games complete, grade the predictions:

```bash
python adaptive_learning/grade_predictions.py 2025-10-31
```

Interactive grading session:
```
Player: Jack Eichel (VGK vs ANA)
Prop: POINTS OVER 0.5
Our Prediction: 72.3% | Tier: T1-ELITE
================================================================================

Actual points: (or 's' to skip, 'q' to quit): 2

âœ… HIT! (2.0 > 0.5)
```

Features:
- Resume where you left off if you quit
- Skip predictions if you don't have the data
- Shows summary at the end

### Step 3: Run Learning Engine

After grading (need at least 30 predictions), run the learning engine:

```bash
python adaptive_learning/learning_engine.py
```

Output:
```
================================================================================
ADAPTIVE LEARNING ENGINE
================================================================================
Found 58 graded predictions to learn from

[*] Learning home ice advantage...
  âœ… points: Home=72.0%, Away=65.0%
     Adjustment: +3.5% for home games (p=0.0234)

[*] Learning player-specific calibration...
  âœ… Connor McDavid: 35 predictions, 77.1% accuracy
     Predicted 70.2% on average â†’ Adjusting by +3.5%

ğŸ“Š Overall Performance:
   Total Predictions: 58
   Hits: 41
   Accuracy: 70.7%

ğŸ“ Active Learned Adjustments: 3
   Top Adjustments:
   - home_ice_boost               (points      ): +3.5% (confidence: 90%, n=58)
   - player_connor_mcdavid        (all props   ): +3.5% (confidence: 85%, n=35)

âœ… LEARNING COMPLETE
```

The system has now learned adjustments that will automatically apply to future predictions!

### Step 4: Apply Adjustments (Manual for Now)

To test adjustments manually:

```bash
python adaptive_learning/apply_adjustments.py
```

This shows loaded adjustments and tests them.

---

## ğŸ“ File Structure

```
adaptive_learning/
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ schema.sql                 # Database schema (tables, indexes, views)
â”œâ”€â”€ grade_predictions.py       # Interactive grading script
â”œâ”€â”€ learning_engine.py         # Brain: analyzes outcomes, learns adjustments
â”œâ”€â”€ apply_adjustments.py       # Module: applies adjustments to predictions
â””â”€â”€ (future) integration.py    # Auto-applies adjustments in workflow
```

---

## ğŸ”„ Daily Workflow (Future State)

**Once integrated**, the daily workflow will be:

```bash
# Morning: Generate predictions (with adaptive adjustments)
python generate_picks_to_file.py

# Evening: Grade yesterday's predictions
python adaptive_learning/grade_predictions.py

# Overnight: Learn from results
python adaptive_learning/learning_engine.py

# Next morning: Predictions automatically use new adjustments!
```

The system improves itself every day with zero manual tuning.

---

## ğŸ§ª Testing Before Integration

**IMPORTANT:** Don't integrate yet! Test first.

### Phase 1: Collect 1 Week of Data

```bash
# Day 1-7: Grade predictions
python adaptive_learning/grade_predictions.py 2025-10-31
python adaptive_learning/grade_predictions.py 2025-11-01
# ... continue for 7 days
```

Goal: Get 100+ graded predictions

### Phase 2: Run Learning Engine

```bash
python adaptive_learning/learning_engine.py
```

Check output:
- Are adjustments reasonable? (+3% home ice boost is good, +30% is suspicious)
- Are they statistically significant? (p < 0.05)
- Do they make intuitive sense?

### Phase 3: Parallel Predictions

Generate predictions with and without adjustments:

```python
# In your prediction script (manual for now)
from adaptive_learning.apply_adjustments import apply_adaptive_adjustments

base_prob = 0.65
adjusted_prob = apply_adaptive_adjustments(
    base_probability=base_prob,
    player_name='Jack Eichel',
    prop_type='points',
    is_home=True
)

print(f"Base: {base_prob:.1%}")
print(f"Adaptive: {adjusted_prob:.1%}")
```

Track both for 1 week. Compare accuracy.

### Phase 4: Integration (If Successful)

If adaptive predictions are 2-5% more accurate, integrate into workflow.

---

## ğŸ“ˆ Expected Results

### Week 1-2
- **Data Collection Phase**
- System learns basic patterns (home ice, obvious player biases)
- 1-2% accuracy improvement

### Month 1
- **Refinement Phase**
- Player-specific calibrations stabilize
- 2-3% accuracy improvement
- Confidence tiers better calibrated

### Month 2-3
- **Optimization Phase**
- Ensemble weights optimized
- Context-aware adjustments (blowouts, back-to-backs)
- 3-5% accuracy improvement
- ROI increases from 5% â†’ 7-8%

### Month 6+
- **Mature System**
- Self-correcting for drift
- Discovers subtle patterns humans miss
- 5-8% accuracy improvement over baseline
- ROI increases to 8-10%

---

## ğŸ“ What Makes This Powerful

### 1. Statistical Rigor
- Chi-square tests for significance
- Confidence-weighted adjustments
- Minimum sample sizes enforced
- Prevents overfitting on noise

### 2. Continuous Learning
- Updates every day with new data
- Self-correcting (if an adjustment stops working, it's removed)
- No manual tuning required

### 3. Transparency
- Every adjustment is logged with reasoning
- Can see exactly what changed and why
- Full audit trail in `learning_log` table

### 4. Conservative by Design
- Adjustments are confidence-weighted (low confidence = small adjustment)
- Clamped to Â±10% max change (prevents wild swings)
- Requires statistical significance (p < 0.05)
- Needs minimum 30 observations

---

## ğŸ” Viewing Learned Insights

### Via Database Queries

```sql
-- Active adjustments
SELECT factor_name, prop_type, adjustment_value, confidence, sample_size
FROM learned_adjustments
WHERE is_significant = 1
ORDER BY ABS(adjustment_value) DESC;

-- Learning history
SELECT event_type, description, new_value, created_at
FROM learning_log
ORDER BY created_at DESC
LIMIT 20;

-- Accuracy by prop type
SELECT * FROM v_accuracy_by_prop;

-- Home vs away split
SELECT * FROM v_home_away_split;
```

### Via Dashboard (Future)

A dashboard page will show:
- Current active adjustments
- Learning timeline (what changed when)
- Accuracy trends over time
- Calibration curves

---

## ğŸš¨ Troubleshooting

### "No predictions to grade"
Make sure you have predictions in the database for that date:
```sql
SELECT COUNT(*) FROM predictions WHERE game_date = '2025-10-31';
```

### "No significant adjustments learned"
- Need at least 30-50 graded predictions
- Differences must be >3% and statistically significant
- This is normal for the first few days

### "Adjustments seem too large"
- Check sample sizes in `learned_adjustments` table
- If n < 30, that's the issue (need more data)
- Large adjustments (>10%) are automatically clamped

### "System isn't improving accuracy"
- Need at least 2 weeks of data to see trends
- Check if adjustments are being applied (test with `apply_adjustments.py`)
- View learning log to see if adjustments are being created

---

## ğŸ”® Future Enhancements

### Planned Features

1. **Auto-grading via API** - Fetch actual stats from NHL API instead of manual entry
2. **Blowout detection** - Learn how goal margins affect accuracy
3. **Back-to-back games** - Adjust for rest days
4. **Opponent strength** - Learn matchup-specific patterns
5. **Ensemble weight optimization** - Dynamic statistical vs ML mixing
6. **Drift detection** - Alert when model performance degrades
7. **A/B testing framework** - Test adjustments before deploying
8. **Confidence interval learning** - Better uncertainty quantification

### Advanced Learning Modules

- **Goalie matchups** - Learn how specific goalies affect opponent stats
- **Line combinations** - Detect when players benefit from linemates
- **Game context** - Playoff games, rivalry games, etc.
- **Weather factors** - For outdoor games
- **Referee tendencies** - Some refs call more penalties

---

## ğŸ“ Need Help?

**Common Questions:**

**Q: When should I start using adaptive adjustments?**
A: After collecting 100+ graded predictions (about 2 weeks). Test in parallel first.

**Q: How often should I run the learning engine?**
A: Daily, after grading predictions. Can also run weekly for batch learning.

**Q: What if an adjustment seems wrong?**
A: Check the sample size and p-value. If n < 30 or p > 0.05, it's not reliable yet. You can also manually remove adjustments from the database.

**Q: Can I disable certain adjustments?**
A: Yes, set `is_significant = 0` in the `learned_adjustments` table for that adjustment.

**Q: Does this replace the ML model?**
A: No, it enhances both statistical and ML models. Think of it as a final calibration layer on top of your existing predictions.

---

## ğŸ‰ You Built a Self-Improving System!

This adaptive learning system will:
- Get smarter every day
- Find patterns you never would have manually coded
- Continuously improve ROI without manual tuning
- Provide transparency into what it's learning

**Welcome to the future of sports betting prediction!** ğŸš€

---

**Next Steps:**
1. Grade 1-2 weeks of predictions
2. Run learning engine
3. Observe learned adjustments
4. Test in parallel with current system
5. Integrate if improvements are real and consistent

**The system is ready. Let the learning begin!**
